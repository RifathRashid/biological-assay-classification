{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "\n",
    "def sign(x, threshold=0):\n",
    "    y = x > threshold\n",
    "    return y.astype(int)\n",
    "\n",
    "def get_data_filenames(data_dir, data_file_ext, assay_name):\n",
    "    '''\n",
    "    Returns dictionary mapping 'train', 'test', and 'score' to the corresponding data filename\n",
    "    '''\n",
    "    return {subfolder: os.path.join(os.getcwd(), data_dir, subfolder, '') + assay_name + '.' + data_file_ext \\\n",
    "            for subfolder in ['train', 'test', 'score']}\n",
    "\n",
    "def read_fingerprint(filename):\n",
    "    '''\n",
    "    Parameters\n",
    "    - filename: str\n",
    "        File must be tab-delimited as follows: smiles code, tox21_id, label, fingerprint\n",
    "    \n",
    "    Returns\n",
    "    - (X, Y): tuple of np.arrays\n",
    "        X is an array of features\n",
    "        Y is a vector of labels\n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    input_file = open(filename, 'r')\n",
    "    \n",
    "    for index, line in enumerate(input_file):\n",
    "        # split line (1 data point) into smiles, fingerprint (features), and label\n",
    "        split_line = line.strip().split('\\t')\n",
    "        # print(index)\n",
    "        # smiles = split_line[0]\n",
    "        fingerprint = [int(c) for c in split_line[3]]\n",
    "        label = int(split_line[2])\n",
    "        \n",
    "        # append data point to train_x (features) and train_y (labels)\n",
    "        X.append(fingerprint)\n",
    "        Y.append(label)\n",
    "    input_file.close()\n",
    "    return (np.array(X), np.array(Y))\n",
    "\n",
    "def read_features(filename):\n",
    "    '''\n",
    "    Parameters\n",
    "    - filename: str\n",
    "        File must be tab-delimited as follows: smiles code, cid, pubchem_fingerprint, 33 extra features (tab-delimited), label\n",
    "    \n",
    "    Returns\n",
    "    - (X, Y): tuple of np.arrays\n",
    "        X is an array of features\n",
    "        Y is a vector of labels\n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    input_file = open(filename, 'r')\n",
    "    \n",
    "    for index, line in enumerate(input_file):\n",
    "        try:\n",
    "            # split line (1 data point) into smiles, fingerprint (features), 33 extra featues, and label\n",
    "            split_line = line.strip().split()\n",
    "            fingerprint = [int(c) for c in split_line[2]]\n",
    "            label = int(split_line[36])\n",
    "            extra_features = split_line[3:36]\n",
    "            fingerprint.extend(extra_features)\n",
    "\n",
    "            # append data point to X (features) and Y (labels)\n",
    "            X.append(fingerprint)\n",
    "            Y.append(label)\n",
    "        except:\n",
    "            print('failed to parse data point %d' % index)\n",
    "            continue\n",
    "    input_file.close()\n",
    "    return (np.array(X, dtype=float), np.array(Y, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, X_train, Y_train, X_test, Y_test):\n",
    "    '''\n",
    "    Parameters\n",
    "    - model: sklearn model\n",
    "    - X_train: array, shape (n_samples, n_features)\n",
    "        The input data for training.\n",
    "    - Y_train: array, shape (n_samples,) or (n_samples, n_outputs)\n",
    "        The target training values (class labels in classification, real numbers in regression).\n",
    "    - X_test: array, shape (n_samples, n_features)\n",
    "        The input data for testing\n",
    "    - Y_test: array, shape (n_samples,) or (n_samples, n_outputs)\n",
    "        The target testing values (class labels in classification, real numbers in regression).\n",
    "    \n",
    "    Prints\n",
    "    - Training confusion matrix\n",
    "    - Testing confusion matrix\n",
    "    - AUROC\n",
    "    '''\n",
    "    # fit model parameters\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # apply model to train and test data\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    \n",
    "    # print confusion matricies\n",
    "    print('train confusion matrix')\n",
    "    print(confusion_matrix(Y_train, train_predictions))\n",
    "    print('test confusion matrix')\n",
    "    print(confusion_matrix(Y_test, test_predictions))\n",
    "    \n",
    "    # calculate AUROC\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    test_accuracy = sklearn.metrics.accuracy_score(Y_test, test_predictions)\n",
    "    test_auc_roc = sklearn.metrics.roc_auc_score(Y_test, y_score[:,1])\n",
    "    print('Test AUROC: %0.3g' % test_auc_roc)\n",
    "    print('Test accuracy: %0.3g' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to parse data point 1046\n",
      "failed to parse data point 1206\n",
      "failed to parse data point 1294\n",
      "failed to parse data point 2556\n",
      "failed to parse data point 2632\n",
      "failed to parse data point 2951\n",
      "failed to parse data point 2976\n",
      "failed to parse data point 3093\n",
      "failed to parse data point 3094\n",
      "failed to parse data point 3095\n",
      "failed to parse data point 3108\n",
      "failed to parse data point 3109\n",
      "failed to parse data point 3212\n",
      "failed to parse data point 3226\n",
      "failed to parse data point 3316\n",
      "failed to parse data point 3353\n",
      "failed to parse data point 3400\n",
      "failed to parse data point 3401\n",
      "failed to parse data point 3402\n",
      "failed to parse data point 3513\n",
      "failed to parse data point 3514\n",
      "failed to parse data point 3515\n",
      "failed to parse data point 3516\n",
      "failed to parse data point 3517\n",
      "failed to parse data point 3721\n",
      "failed to parse data point 3722\n",
      "failed to parse data point 3796\n",
      "failed to parse data point 4169\n",
      "failed to parse data point 4170\n",
      "failed to parse data point 4171\n",
      "failed to parse data point 4172\n",
      "failed to parse data point 4233\n",
      "failed to parse data point 4234\n",
      "failed to parse data point 4237\n",
      "failed to parse data point 4252\n",
      "failed to parse data point 4361\n",
      "failed to parse data point 4362\n",
      "failed to parse data point 4363\n",
      "failed to parse data point 4558\n",
      "failed to parse data point 4559\n",
      "failed to parse data point 4560\n",
      "failed to parse data point 4625\n",
      "failed to parse data point 4942\n",
      "failed to parse data point 4943\n",
      "failed to parse data point 4944\n",
      "failed to parse data point 4945\n",
      "failed to parse data point 4946\n",
      "failed to parse data point 4947\n",
      "failed to parse data point 4948\n",
      "failed to parse data point 4949\n",
      "failed to parse data point 4983\n",
      "failed to parse data point 5497\n",
      "failed to parse data point 5517\n",
      "failed to parse data point 5518\n",
      "failed to parse data point 5519\n",
      "failed to parse data point 5520\n",
      "failed to parse data point 5521\n",
      "failed to parse data point 5664\n",
      "failed to parse data point 5665\n",
      "failed to parse data point 5666\n",
      "failed to parse data point 5667\n",
      "failed to parse data point 5668\n",
      "failed to parse data point 5717\n",
      "failed to parse data point 5887\n",
      "failed to parse data point 5902\n",
      "failed to parse data point 168\n",
      "failed to parse data point 169\n",
      "failed to parse data point 170\n",
      "failed to parse data point 177\n",
      "failed to parse data point 323\n",
      "failed to parse data point 324\n",
      "failed to parse data point 325\n",
      "failed to parse data point 359\n"
     ]
    }
   ],
   "source": [
    "assay_name = 'nr-ahr'\n",
    "data_dir = 'data_pcfp_ext'\n",
    "data_file_ext = 'features'\n",
    "\n",
    "filenames = get_data_filenames(data_dir, data_file_ext, assay_name)\n",
    "X_train, Y_train = read_features(filenames['train'])\n",
    "X_test, Y_test = read_features(filenames['test'])\n",
    "X_score, Y_score = read_features(filenames['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.naive_bayes.GaussianNB'>\n",
      "train confusion matrix\n",
      "[[3762 1709]\n",
      " [  83  712]]\n",
      "test confusion matrix\n",
      "[[55 76]\n",
      " [ 3 16]]\n",
      "Test AUROC: 0.648\n",
      "Test accuracy: 0.473\n",
      "\n",
      "---\n",
      "\n",
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "train confusion matrix\n",
      "[[3764 1707]\n",
      " [ 122  673]]\n",
      "test confusion matrix\n",
      "[[57 74]\n",
      " [ 4 15]]\n",
      "Test AUROC: 0.671\n",
      "Test accuracy: 0.48\n",
      "\n",
      "---\n",
      "\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "train confusion matrix\n",
      "[[5470    1]\n",
      " [  40  755]]\n",
      "test confusion matrix\n",
      "[[104  27]\n",
      " [ 12   7]]\n",
      "Test AUROC: 0.6\n",
      "Test accuracy: 0.74\n",
      "\n",
      "---\n",
      "\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "train confusion matrix\n",
      "[[5454   17]\n",
      " [  63  732]]\n",
      "test confusion matrix\n",
      "[[117  14]\n",
      " [ 15   4]]\n",
      "Test AUROC: 0.694\n",
      "Test accuracy: 0.807\n",
      "\n",
      "---\n",
      "\n",
      "<class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>\n",
      "train confusion matrix\n",
      "[[5150  321]\n",
      " [ 175  620]]\n",
      "test confusion matrix\n",
      "[[94 37]\n",
      " [ 5 14]]\n",
      "Test AUROC: 0.79\n",
      "Test accuracy: 0.72\n",
      "\n",
      "---\n",
      "\n",
      "<class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>\n",
      "train confusion matrix\n",
      "[[5230  241]\n",
      " [ 323  472]]\n",
      "test confusion matrix\n",
      "[[103  28]\n",
      " [ 11   8]]\n",
      "Test AUROC: 0.74\n",
      "Test accuracy: 0.74\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3_64\\envs\\cs221_project_py36x64\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Comparing different models\n",
    "models = []\n",
    "models.append(GaussianNB())  # Naive Bayes, Gaussian\n",
    "models.append(BernoulliNB()) # Naive Bayes, Bernouilli\n",
    "models.append(DecisionTreeClassifier()) # Decision tree\n",
    "models.append(RandomForestClassifier()) # Random forest\n",
    "models.append(MLPClassifier()) # Neural network\n",
    "models.append(MLPClassifier(hidden_layer_sizes=(512,),activation='relu',batch_size=100,alpha=0.1,max_iter=4)) # custom neural net\n",
    "\n",
    "for model in models:\n",
    "    print(type(model))\n",
    "    train_and_test(model, X_train, Y_train, X_test, Y_test)\n",
    "    print('\\n---\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
