{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# structure of code largely based on tensorflow tutorial for deep neural nets: https://www.tensorflow.org/get_started/mnist/pros\n",
    "# see full code of tutorial here: https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/tutorials/mnist/mnist_deep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "\n",
    "def sign(x, threshold=0):\n",
    "    y = x > threshold\n",
    "    return y.astype(int)\n",
    "\n",
    "def get_data_filenames(data_dir, data_file_ext, assay_name):\n",
    "    '''\n",
    "    Returns dictionary mapping 'train', 'test', and 'score' to the corresponding data filename\n",
    "    '''\n",
    "    return {subfolder: os.path.join(os.getcwd(), data_dir, subfolder, '') + assay_name + '.' + data_file_ext \\\n",
    "            for subfolder in ['train', 'test', 'score']}\n",
    "\n",
    "def read_file(filename):\n",
    "    '''\n",
    "    Parameters\n",
    "    - filename: str\n",
    "        File must be tab-delimited as follows: smiles code, tox21_id, label, fingerprint\n",
    "    \n",
    "    Returns\n",
    "    - (X, Y): tuple of np.arrays\n",
    "        X is an array of features\n",
    "        Y is a vector of labels\n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    input_file = open(filename, 'r')\n",
    "    \n",
    "    for index, line in enumerate(input_file):\n",
    "        # split line (1 data point) into smiles, fingerprint (features), and label\n",
    "        split_line = line.strip().split('\\t')\n",
    "        # print(index)\n",
    "        # smiles = split_line[0]\n",
    "        fingerprint = [int(c) for c in split_line[3]]\n",
    "        label = int(split_line[2])\n",
    "        \n",
    "        # append data point to train_x (features) and train_y (labels)\n",
    "        X.append(fingerprint)\n",
    "        Y.append(label)\n",
    "    input_file.close()\n",
    "    return (np.array(X), np.array(Y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## if running as main function\n",
    "\n",
    "# construct parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--run_id', type=str, default='', help='run id')\n",
    "parser.add_argument('--rand_seed', type=int, default='848', help='graph-level random seed for tensorflow')\n",
    "parser.add_argument('--assay_name', type=str, required=True, help='assay name, e.g. nr-ar, sr-are, ...')\n",
    "parser.add_argument('--data_dir', type=str, required=True, help='name of directory to find train, test, and score data files')\n",
    "parser.add_argument('--data_file_ext', type=str, default='data', help='file extension, exluduing the period (e.g. ''fp'', ''data'', etc)')\n",
    "parser.add_argument('--loss_balance', action='store_true', help='adjust loss function to account for unbalanced dataset, default = false')\n",
    "parser.add_argument('--kernel_reg_const', type=float, default=0.1, help='L2 kernel regularization constant')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='batch size. default = 1 (SGD)')\n",
    "parser.add_argument('--num_epochs', type=int, default=1, help='number of epochs (passes through entire training set)')\n",
    "parser.add_argument('--node_array', nargs='*', required=True, help='sizes of hidden layers in the neural network. use 0 for a simple linear classifier')\n",
    "\n",
    "# parse arguments\n",
    "args = parser.parse_args()\n",
    "rand_seed = args.rand_seed\n",
    "run_id = args.run_id\n",
    "assay_name = args.assay_name\n",
    "data_dir = args.data_dir\n",
    "data_file_ext = args.data_file_ext.lstrip('.')\n",
    "loss_balance = args.loss_balance\n",
    "kernel_reg_const = args.kernel_reg_const\n",
    "batch_size = args.batch_size\n",
    "num_epochs = args.num_epochs\n",
    "node_array = np.array(args.node_array, dtype=int)\n",
    "\n",
    "# get data\n",
    "filenames = get_data_filenames(data_dir, data_file_ext, assay_name)\n",
    "X_train, Y_train = read_file(filenames['train'])\n",
    "X_test, Y_test = read_file(filenames['test'])\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if running inside iPython notebook\n",
    "\n",
    "# parameters\n",
    "rand_seed = 848\n",
    "run_id = 1\n",
    "assay_name = 'nr-ahr'\n",
    "data_dir = 'fingerprints'\n",
    "data_file_ext = 'fp'\n",
    "loss_balance = True\n",
    "kernel_reg_const = 0.1\n",
    "batch_size = 50\n",
    "num_epochs = 3\n",
    "node_array = np.array([512, 256, 128])\n",
    "\n",
    "params={'run_id': run_id,\n",
    "        'rand_seed': rand_seed,\n",
    "        'assay_name': assay_name,\n",
    "        'data_dir': data_dir,\n",
    "        'data_file_ext': data_file_ext,\n",
    "        'loss_balance': loss_balance,\n",
    "        'kernel_reg_const': kernel_reg_const,\n",
    "        'batch_size': batch_size,\n",
    "        'num_epochs': num_epochs,\n",
    "        'node_array': node_array}\n",
    "\n",
    "# get data\n",
    "filenames = get_data_filenames(data_dir, data_file_ext, assay_name)\n",
    "X_train, Y_train = read_file(filenames['train'])\n",
    "X_test, Y_test = read_file(filenames['test'])\n",
    "num_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Model - basic ##\n",
    "\n",
    "# Notes\n",
    "# Probability of classifying into the positive class = sigmoid(logit)\n",
    "# logit can take on any real value\n",
    "\n",
    "def deepnn_params(x, nodes, kernel_reg_const=0.1):\n",
    "    \"\"\"\n",
    "    deepnn builds the graph for a deep net for learning the logit\n",
    "\n",
    "    Args:\n",
    "        x: input layer. type = tf.Tensor. size = (batch_size, num_features)\n",
    "        nodes: a list of number of nodes in hidden layers. type = np.ndarray\n",
    "        kernel_reg_const: L2 regularization weights. type = float\n",
    "\n",
    "    Returns:\n",
    "        y: a tensor of length batch_size with values equal to the logits\n",
    "            of classifying an input data point into the positive class\n",
    "    \"\"\"\n",
    "    # tensorflow dense layer example: https://www.tensorflow.org/tutorials/layers#dense_layer\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(x)\n",
    "    \n",
    "    num_hidden_layers = min(nodes.size,nodes[0])\n",
    "    for i in range(num_hidden_layers):\n",
    "        layers.append(tf.layers.dense(inputs=layers[i], units=nodes[i], activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l2_regularizer(kernel_reg_const)))\n",
    "    layers.append(tf.layers.dense(inputs=layers[num_hidden_layers], units=1, activation=None, kernel_regularizer=tf.contrib.layers.l2_regularizer(kernel_reg_const)))\n",
    "    return tf.squeeze(layers[-1])\n",
    "\n",
    "# sign tensorflow function\n",
    "def sign_tf(x, threshold=0):\n",
    "    return tf.cast(tf.greater_equal(x, threshold), tf.int32)\n",
    "\n",
    "# input\n",
    "x = tf.placeholder(tf.float32, [None, num_features])\n",
    "\n",
    "# labels\n",
    "y_labels = tf.placeholder(tf.float32, [None]) # domain: {0,1}\n",
    "\n",
    "# loss weights for unbalanced data\n",
    "q = tf.placeholder(tf.float32, None)\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_score = deepnn_params(x, node_array, kernel_reg_const)\n",
    "y_prob = tf.sigmoid(y_score)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# logistic loss, aka sigmoid cross entropy\n",
    "# y * -log(sigmoid(x)) + (1 - y) * -log(1 - sigmoid(x)), where x is the logit and y is the label\n",
    "loss_fn = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(targets=y_labels, logits=y_score, pos_weight=q))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss_fn)\n",
    "\n",
    "correct_prediction = tf.equal(sign_tf(y_score), tf.cast(y_labels, tf.int32))\n",
    "correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train model ##\n",
    "\n",
    "np.random.seed(rand_seed)\n",
    "\n",
    "# calculate frequencies of positives, negatives in training set\n",
    "# - https://stackoverflow.com/questions/35155655/loss-function-for-class-imbalanced-binary-classifier-in-tensor-flow\n",
    "q_train = Y_train.size/np.sum(Y_train)\n",
    "if not loss_balance:\n",
    "    q_train = 1\n",
    "\n",
    "# training parameters\n",
    "num_batches_per_epoch = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(rand_seed)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle indices of training data\n",
    "    shuffle_indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "\n",
    "    for i in range(num_batches_per_epoch):\n",
    "        # get batch\n",
    "        batch_indices = shuffle_indices[i*batch_size : (i+1)*batch_size]\n",
    "        batch_x = X_train[batch_indices]\n",
    "        batch_y = Y_train[batch_indices]\n",
    "\n",
    "        # train on batch data\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y_labels: batch_y, q: q_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUROC - sklearn\n",
    "\n",
    "train_accuracy, train_loss = sess.run([accuracy, loss_fn], feed_dict={x: X_train, y_labels: Y_train, q: q_train})\n",
    "\n",
    "# get normalized score, i.e. probability of classifying into positive class\n",
    "y_prob_test = sess.run(y_prob, feed_dict={x: X_test})\n",
    "test_accuracy = sess.run(accuracy, feed_dict={x: X_test, y_labels: Y_test})\n",
    "\n",
    "fpr, tpr, thresholds = sk.metrics.roc_curve(Y_test, y_prob_test)\n",
    "auc_roc = sk.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save parameters, accuracy, and auc_roc\n",
    "results_file = os.path.join(os.getcwd(), 'results','') + str(run_id) + '.results'\n",
    "\n",
    "params['accuracy'] = test_accuracy\n",
    "params['auc_roc'] = auc_roc\n",
    "params['train_loss'] = train_loss\n",
    "params['train_accuracy'] = train_accuracy\n",
    "\n",
    "series = pd.Series(params)\n",
    "df = pd.DataFrame(series)\n",
    "df = df.T\n",
    "df.to_csv(results_file, index=False)\n",
    "# series.to_csv(results_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
